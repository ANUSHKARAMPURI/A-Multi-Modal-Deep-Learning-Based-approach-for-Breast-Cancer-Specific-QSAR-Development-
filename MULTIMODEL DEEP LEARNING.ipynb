{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from padelpy import from_smiles\n",
    "\n",
    "def calculate_molecular_descriptors(descriptor_list, output_filename='all_descriptors.csv'):\n",
    "    \"\"\"\n",
    "    Calculate molecular descriptors using padelpy from a list of SMILES IDs.\n",
    "\n",
    "    Parameters:\n",
    "    - descriptor_list (list): List of SMILES IDs.\n",
    "    - output_filename (str, optional): Output filename to save the descriptors (default is 'all_descriptors.csv').\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves the calculated descriptors to a CSV file.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame to store all the descriptors\n",
    "    full_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each SMILES ID\n",
    "    for index, smile_id in enumerate(descriptor_list):\n",
    "        print(f\"Calculating descriptors for SMILES ID {index + 1}/{len(descriptor_list)}\")\n",
    "        output_csv = f'{index}.csv'\n",
    "        from_smiles(smile_id, fingerprints=False, output_csv=output_csv, timeout=600)\n",
    "        df = pd.read_csv(output_csv)\n",
    "        full_df = pd.concat([full_df, df], ignore_index=True)\n",
    "\n",
    "    # Save all descriptors to a single CSV file\n",
    "    full_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Descriptors saved to {output_filename}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your list of SMILES IDs\n",
    "    descriptor_list = ['SMILES_ID_1', 'SMILES_ID_2', 'SMILES_ID_3']  # Example SMILES IDs\n",
    "    \n",
    "    # Call the function to calculate descriptors\n",
    "    calculate_molecular_descriptors(descriptor_list, output_filename='all_descriptors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d54b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis, boxcox, yeojohnson\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from MultiColumnLabelEncoder import MultiColumnLabelEncoder\n",
    "\n",
    "def preprocess_multi_omics_datasets(transcriptomics_df, proteomics_df, fusion_protein_df, crispr_df, cn_alteration_df, mutation_df):\n",
    "    \"\"\"\n",
    "    Preprocess multiple omics datasets for drug designing and model development.\n",
    "    \n",
    "    Parameters:\n",
    "    transcriptomics_df (DataFrame): Transcriptomics dataset.\n",
    "    proteomics_df (DataFrame): Proteomics dataset.\n",
    "    fusion_protein_df (DataFrame): Fusion Protein dataset.\n",
    "    crispr_df (DataFrame): CRISPR KO dataset.\n",
    "    cn_alteration_df (DataFrame): Copy number alteration dataset.\n",
    "    mutation_df (DataFrame): Mutation dataset.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Preprocessed DataFrames for each omics dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocess_single_dataset(df, dataset_name):\n",
    "        \"\"\"\n",
    "        Preprocess a single omics dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        df (DataFrame): Input dataset.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: Preprocessed dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Preprocessing {dataset_name} dataset...\")\n",
    "        \n",
    "        # Handling missing values\n",
    "        print(f\"Handling missing values for {dataset_name} dataset...\")\n",
    "        def fill_null_with_mean(df):\n",
    "            df.fillna(df.mean(), inplace=True)\n",
    "            df.fillna(0, inplace=True)\n",
    "        \n",
    "        fill_null_with_mean(df)\n",
    "        \n",
    "        # Encoding categorical variables\n",
    "        print(f\"Encoding categorical variables for {dataset_name} dataset...\")\n",
    "        mcle = MultiColumnLabelEncoder()\n",
    "        df = mcle.fit_transform(df)\n",
    "        \n",
    "        # Identifying skewness\n",
    "        print(f\"Identifying skewness for {dataset_name} dataset...\")\n",
    "        skewness_values = np.apply_along_axis(skew, axis=0, arr=df)\n",
    "        plt.hist(x=skewness_values)\n",
    "        plt.title(f'Skewness Distribution for {dataset_name}')\n",
    "        plt.show()\n",
    "\n",
    "        # Identifying kurtosis\n",
    "        print(f\"Identifying kurtosis for {dataset_name} dataset...\")\n",
    "        kurtosis_values = np.apply_along_axis(kurtosis, axis=0, arr=df)\n",
    "        plt.hist(x=kurtosis_values)\n",
    "        plt.title(f'Kurtosis Distribution for {dataset_name}')\n",
    "        plt.show()\n",
    "\n",
    "        # Transforming columns to reduce skewness and kurtosis\n",
    "        print(f\"Transforming columns to reduce skewness and kurtosis for {dataset_name} dataset...\")\n",
    "        start_column = 12  # Adjust as per your dataset\n",
    "        \n",
    "        X = df.iloc[:, start_column:]\n",
    "        kurtosis_threshold = 3\n",
    "        \n",
    "        for i, kurt in enumerate(kurtosis_values[start_column:]):\n",
    "            col_index = start_column + i\n",
    "            if kurt > kurtosis_threshold:\n",
    "                try:\n",
    "                    transformed_column, _ = boxcox(X.iloc[:, i] + 1)\n",
    "                    X.iloc[:, i] = transformed_column\n",
    "                except ValueError:\n",
    "                    transformed_column, _ = yeojohnson(X.iloc[:, i] + 1)\n",
    "                    X.iloc[:, i] = transformed_column\n",
    "            elif kurt < kurtosis_threshold:\n",
    "                if np.min(X.iloc[:, i]) >= 0:\n",
    "                    X.iloc[:, i] = np.sqrt(X.iloc[:, i])\n",
    "                else:\n",
    "                    X.iloc[:, i] = np.log(X.iloc[:, i] - np.min(X.iloc[:, i]) + 1)\n",
    "        \n",
    "        df.iloc[:, start_column:] = X\n",
    "        \n",
    "        print(f\"Preprocessing for {dataset_name} dataset complete.\")\n",
    "        return df\n",
    "    \n",
    "    # Process each dataset\n",
    "    processed_transcriptomics = preprocess_single_dataset(transcriptomics_df, \"Transcriptomics\")\n",
    "    processed_proteomics = preprocess_single_dataset(proteomics_df, \"Proteomics\")\n",
    "    processed_fusion_protein = preprocess_single_dataset(fusion_protein_df, \"Fusion Protein\")\n",
    "    processed_crispr = preprocess_single_dataset(crispr_df, \"CRISPR KO\")\n",
    "    processed_cn_alteration = preprocess_single_dataset(cn_alteration_df, \"Copy Number Alteration\")\n",
    "    processed_mutation = preprocess_single_dataset(mutation_df, \"Mutation\")\n",
    "    \n",
    "    print(\"Data preprocessing for all datasets complete.\")\n",
    "    \n",
    "    return (processed_transcriptomics, processed_proteomics, processed_fusion_protein, processed_crispr, processed_cn_alteration, processed_mutation)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your datasets here, replace with actual dataset loading code\n",
    "    transcriptomics_df = pd.DataFrame()  # Replace with actual data loading\n",
    "    proteomics_df = pd.DataFrame()  # Replace with actual data loading\n",
    "    fusion_protein_df = pd.DataFrame()  # Replace with actual data loading\n",
    "    crispr_df = pd.DataFrame()  # Replace with actual data loading\n",
    "    cn_alteration_df = pd.DataFrame()  # Replace with actual data loading\n",
    "    mutation_df = pd.DataFrame()  # Replace with actual data loading\n",
    "    \n",
    "    # Preprocess all datasets\n",
    "    (processed_transcriptomics, processed_proteomics, processed_fusion_protein,\n",
    "     processed_crispr, processed_cn_alteration, processed_mutation) = preprocess_multi_omics_datasets(transcriptomics_df,\n",
    "                                                                                                      proteomics_df,\n",
    "                                                                                                      fusion_protein_df,\n",
    "                                                                                                      crispr_df,\n",
    "                                                                                                      cn_alteration_df,\n",
    "                                                                                                      mutation_df)\n",
    "    \n",
    "    # Example of accessing processed data\n",
    "    print(\"Processed Transcriptomics Data:\")\n",
    "    print(processed_transcriptomics.head())\n",
    "    # Similarly, access other processed datasets as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "epsilon = 1e-8\n",
    "\n",
    "def relu(x):\n",
    "    return T.switch(x < 0, 0, x)\n",
    "\n",
    "def create_weight(dim_input, dim_output, sigma_init=0.01):\n",
    "    return np.random.normal(0, sigma_init, (dim_input, dim_output)).astype(theano.config.floatX)\n",
    "\n",
    "def create_bias(dim_output):\n",
    "    return np.zeros(dim_output).astype(theano.config.floatX)\n",
    "\n",
    "def variational_autoencoder(x_train, continuous=True, hu_encoder=500, hu_decoder=500, n_latent=20, \n",
    "                            b1=0.95, b2=0.999, batch_size=100, learning_rate=0.001, lam=0, L=1):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder for dimensionality reduction of omics datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train (numpy.ndarray): Input data matrix of shape (N, features) where N is the number of samples.\n",
    "    - continuous (bool, optional): Whether the data is continuous (default is True).\n",
    "    - hu_encoder (int, optional): Number of units in the encoder hidden layer (default is 500).\n",
    "    - hu_decoder (int, optional): Number of units in the decoder hidden layer (default is 500).\n",
    "    - n_latent (int, optional): Dimensionality of the latent space (default is 20).\n",
    "    - b1 (float, optional): Adam optimizer parameter (default is 0.95).\n",
    "    - b2 (float, optional): Adam optimizer parameter (default is 0.999).\n",
    "    - batch_size (int, optional): Batch size for training (default is 100).\n",
    "    - learning_rate (float, optional): Learning rate for training (default is 0.001).\n",
    "    - lam (float, optional): L2 regularization coefficient (default is 0).\n",
    "    - L (int, optional): Number of samples z^(i,l) per datapoint (default is 1).\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: Transformed data matrix of shape (N, n_latent).\n",
    "    \"\"\"\n",
    "    [N, features] = x_train.shape\n",
    "    prng = np.random.RandomState(42)\n",
    "\n",
    "    # Define weights and biases\n",
    "    W_xh = theano.shared(create_weight(features, hu_encoder), name='W_xh')\n",
    "    b_xh = theano.shared(create_bias(hu_encoder), name='b_xh')\n",
    "\n",
    "    W_hmu = theano.shared(create_weight(hu_encoder, n_latent), name='W_hmu')\n",
    "    b_hmu = theano.shared(create_bias(n_latent), name='b_hmu')\n",
    "\n",
    "    W_hsigma = theano.shared(create_weight(hu_encoder, n_latent), name='W_hsigma')\n",
    "    b_hsigma = theano.shared(create_bias(n_latent), name='b_hsigma')\n",
    "\n",
    "    W_zh = theano.shared(create_weight(n_latent, hu_decoder), name='W_zh')\n",
    "    b_zh = theano.shared(create_bias(hu_decoder), name='b_zh')\n",
    "\n",
    "    params = OrderedDict([\n",
    "        (\"W_xh\", W_xh), (\"b_xh\", b_xh), (\"W_hmu\", W_hmu), (\"b_hmu\", b_hmu),\n",
    "        (\"W_hsigma\", W_hsigma), (\"b_hsigma\", b_hsigma), (\"W_zh\", W_zh), (\"b_zh\", b_zh)\n",
    "    ])\n",
    "\n",
    "    if continuous:\n",
    "        W_hxmu = theano.shared(create_weight(hu_decoder, features), name='W_hxmu')\n",
    "        b_hxmu = theano.shared(create_bias(features), name='b_hxmu')\n",
    "\n",
    "        W_hxsig = theano.shared(create_weight(hu_decoder, features), name='W_hxsigma')\n",
    "        b_hxsig = theano.shared(create_bias(features), name='b_hxsigma')\n",
    "\n",
    "        params.update({'W_hxmu': W_hxmu, 'b_hxmu': b_hxmu, 'W_hxsigma': W_hxsig, 'b_hxsigma': b_hxsig})\n",
    "    else:\n",
    "        W_hx = theano.shared(create_weight(hu_decoder, features), name='W_hx')\n",
    "        b_hx = theano.shared(create_bias(features), name='b_hx')\n",
    "\n",
    "        params.update({'W_hx': W_hx, 'b_hx': b_hx})\n",
    "\n",
    "    m = OrderedDict()\n",
    "    v = OrderedDict()\n",
    "\n",
    "    for key, value in params.items():\n",
    "        m[key] = theano.shared(np.zeros_like(value.get_value()).astype(theano.config.floatX), name='m_' + key)\n",
    "        v[key] = theano.shared(np.zeros_like(value.get_value()).astype(theano.config.floatX), name='v_' + key)\n",
    "\n",
    "    x_train = theano.shared(x_train.astype(theano.config.floatX), name=\"x_train\")\n",
    "\n",
    "    def encoder(x):\n",
    "        h_encoder = relu(T.dot(x, params['W_xh']) + params['b_xh'].dimshuffle('x', 0))\n",
    "        mu = T.dot(h_encoder, params['W_hmu']) + params['b_hmu'].dimshuffle('x', 0)\n",
    "        log_sigma = T.dot(h_encoder, params['W_hsigma']) + params['b_hsigma'].dimshuffle('x', 0)\n",
    "        return mu, log_sigma\n",
    "\n",
    "    def sampler(mu, log_sigma):\n",
    "        seed = 42\n",
    "        srng = theano.tensor.shared_randomstreams.RandomStreams(seed=seed)\n",
    "        eps = srng.normal((L, mu.shape[0], n_latent))\n",
    "        z = mu + T.exp(0.5 * log_sigma) * eps\n",
    "        return z\n",
    "\n",
    "    def decoder(x, z):\n",
    "        h_decoder = relu(T.dot(z, params['W_zh']) + params['b_zh'].dimshuffle('x', 0))\n",
    "        if continuous:\n",
    "            reconstructed_x = T.dot(h_decoder, params['W_hxmu']) + params['b_hxmu'].dimshuffle('x', 0)\n",
    "            log_sigma_decoder = T.dot(h_decoder, params['W_hxsigma']) + params['b_hxsigma']\n",
    "            logpxz = (-(0.5 * np.log(2 * np.pi) + 0.5 * log_sigma_decoder) -\n",
    "                      0.5 * ((x - reconstructed_x)**2 / T.exp(log_sigma_decoder))).sum(axis=2).mean(axis=0)\n",
    "        else:\n",
    "            reconstructed_x = T.nnet.sigmoid(T.dot(h_decoder, params['W_hx']) + params['b_hx'].dimshuffle('x', 0))\n",
    "            logpxz = - T.nnet.binary_crossentropy(reconstructed_x, x).sum(axis=2).mean(axis=0)\n",
    "        return reconstructed_x, logpxz\n",
    "\n",
    "    def create_gradientfunctions(x_train):\n",
    "        x = T.matrix(\"x\")\n",
    "        epoch = T.scalar(\"epoch\")\n",
    "        batch_size = x.shape[0]\n",
    "        mu, log_sigma = encoder(x)\n",
    "        z = sampler(mu, log_sigma)\n",
    "        reconstructed_x, logpxz = decoder(x, z)\n",
    "        KLD = 0.5 * T.sum(1 + log_sigma - mu**2 - T.exp(log_sigma), axis=1)\n",
    "        logpx = T.mean(logpxz + KLD)\n",
    "        gradients = T.grad(logpx, list(params.values()))\n",
    "        updates = get_adam_updates(gradients, epoch)\n",
    "        batch = T.iscalar('batch')\n",
    "        givens = {\n",
    "            x: x_train[batch * batch_size:(batch + 1) * batch_size, :]\n",
    "        }\n",
    "        update = theano.function([batch, epoch], logpx, updates=updates, givens=givens)\n",
    "        likelihood = theano.function([x], logpx)\n",
    "        encode = theano.function([x], z)\n",
    "        decode = theano.function([z], reconstructed_x)\n",
    "        return update, likelihood, encode, decode\n",
    "\n",
    "    def transform_data(x_train):\n",
    "        transformed_x = np.zeros((N, n_latent))\n",
    "        batches = np.arange(int(N / batch_size))\n",
    "        for batch in batches:\n",
    "            batch_x = x_train[batch * batch_size:(batch + 1) * batch_size, :]\n",
    "            transformed_x[batch * batch_size:(batch + 1) * batch_size, :] = encode(batch_x)\n",
    "        return transformed_x\n",
    "\n",
    "    def save_parameters(path):\n",
    "        pickle.dump({name: p.get_value() for name, p in params.items()}, open(path + \"/params.pkl\", \"wb\"))\n",
    "        pickle.dump({name: m.get_value() for name, m in m.items()}, open(path + \"/m.pkl\", \"wb\"))\n",
    "        pickle.dump({name: v.get_value() for name, v in v.items()}, open(path + \"/v.pkl\", \"wb\"))\n",
    "\n",
    "    def load_parameters(path):\n",
    "        p_list = pickle.load(open(path + \"/params.pkl\", \"rb\"))\n",
    "        m_list = pickle.load(open(path + \"/m.pkl\", \"rb\"))\n",
    "        v_list = pickle.load(open(path + \"/v.pkl\", \"rb\"))\n",
    "        for name in p_list.keys():\n",
    "            params[name].set_value(p_list[name].astype(theano.config.floatX))\n",
    "            m[name].set_value(m_list[name].astype(theano.config.floatX))\n",
    "            v[name].set_value(v_list[name].astype(theano.config.floatX))\n",
    "\n",
    "    def get_adam_updates(gradients, epoch):\n",
    "        updates = OrderedDict()\n",
    "        gamma = T.sqrt(1 - b2 ** epoch) / (1 - b1 ** epoch)\n",
    "        values_iterable = zip(params.keys(), params.values(), gradients, m.values(), v.values())\n",
    "        for name, parameter, gradient, m, v in values_iterable:\n",
    "            new_m = b1 * m + (1. - b1) * gradient\n",
    "            new_v = b2 * v + (1. - b2) * (gradient ** 2)\n",
    "            updates[parameter] = parameter + learning_rate * gamma * new_m / (T.sqrt(new_v) + epsilon)\n",
    "            if 'W' in name:\n",
    "                updates[parameter] -= learning_rate * lam * (parameter * np.float32(batch_size / N))\n",
    "            updates[m] = new_m\n",
    "            updates[v] = new_v\n",
    "        return updates\n",
    "\n",
    "    update, likelihood, encode, decode = create_gradientfunctions(x_train)\n",
    "\n",
    "    # Training loop (if needed)\n",
    "\n",
    "    # Example usage:\n",
    "    # transformed_data = transform_data(x_train)\n",
    "    # save_parameters(\"model_path\")\n",
    "    # load_parameters(\"model_path\")\n",
    "    \n",
    "    return transform_data(x_train)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your omics dataset\n",
    "    x_train = np.random.rand(100, 500)  # Example dataset (replace with your data)\n",
    "    \n",
    "    # Perform dimensionality reduction using VAE\n",
    "    transformed_data = variational_autoencoder(x_train)\n",
    "    \n",
    "    # Save or use the transformed_data as needed\n",
    "    print(f\"Transformed data shape: {transformed_data.shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bcaac30",
   "metadata": {},
   "source": [
    "  Explanation:\n",
    "Function variational_autoencoder:\n",
    "Takes x_train as input, which is a numpy array of shape (N, features) representing the omics dataset.\n",
    "Initializes parameters, defines encoder, sampler, decoder, gradient functions, and other necessary functions within the scope of the VAE.\n",
    "Performs dimensionality reduction using VAE and returns the transformed data matrix of shape (N, n_latent).\n",
    "Usage:\n",
    "Replace x_train with your actual omics dataset.\n",
    "Call variational_autoencoder(x_train) to perform dimensionality reduction.\n",
    "Adjust parameters (continuous, hu_encoder, hu_decoder, n_latent, etc.) as needed for your specific application.\n",
    "This refactoring encapsulates the VAE functionality into a modular function, making it easier to integrate into different workflows or pipelines for omics data analysis. Adjustments to handling null values can be integrated into the input data preparation before passing it to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eced3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import GaussianNoise, BatchNormalization, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_denoising_autoencoder(input_shape):\n",
    "    \"\"\"\n",
    "    Build and train a denoising autoencoder model for dimensionality reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of the input data.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Trained autoencoder model, encoder model, and encoded data DataFrame.\n",
    "    \"\"\"\n",
    "    # Define the input layer\n",
    "    input_data = keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Add Gaussian noise to the input data\n",
    "    noisy_input = GaussianNoise(0.25)(input_data)  # Adjust the standard deviation for more substantial noise\n",
    "    \n",
    "    # Encoder layers\n",
    "    encoded = layers.Dense(2000, activation='relu')(noisy_input)\n",
    "    encoded = Dropout(0.2)(encoded)  # Adding Dropout for regularization\n",
    "    \n",
    "    # Decoder layers\n",
    "    decoded = layers.Dense(input_shape[0], activation='relu')(encoded)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    autoencoder = keras.Model(input_data, decoded)\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    # Compile the autoencoder\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(X_resampled, X_resampled,\n",
    "                    epochs=50,\n",
    "                    batch_size=25,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.3,\n",
    "                    callbacks=[EarlyStopping('val_loss', patience=10)])\n",
    "    \n",
    "    # Define the encoder model (up to the bottleneck layer)\n",
    "    encoder = keras.Model(input_data, encoded)\n",
    "    \n",
    "    # Define the decoder model\n",
    "    bottleneck_input = keras.Input(shape=(2000,))\n",
    "    decoder_layers = autoencoder.layers[-1]\n",
    "    decoder_output = bottleneck_input\n",
    "    \n",
    "    # Adding BatchNormalization and Dropout in the decoder\n",
    "    decoder_output = BatchNormalization()(decoder_output)\n",
    "    decoder_output = Dropout(0.2)(decoder_output)\n",
    "    \n",
    "    decoder_output = decoder_layers(decoder_output)\n",
    "    decoder = keras.Model(bottleneck_input, decoder_output)\n",
    "    \n",
    "    # Get the encoded data\n",
    "    encoded_data = encoder.predict(X_resampled)\n",
    "    encoded_df = pd.DataFrame(encoded_data)\n",
    "    \n",
    "    return autoencoder, encoder, encoded_df\n",
    "\n",
    "# Assuming X_resampled is your input data\n",
    "X_resampled = pd.DataFrame()  # Replace with actual data\n",
    "\n",
    "# Build the denoising autoencoder\n",
    "autoencoder, encoder, encoded_df = build_denoising_autoencoder(input_shape=(20532,))\n",
    "\n",
    "# Example code to concatenate multiomics datasets after dimensionality reduction\n",
    "# Concatenate encoded data with other omics datasets\n",
    "concatenated_df = pd.concat([encoded_df, transcriptomics_df, proteomics_df, fusion_protein_df,\n",
    "                             crispr_df, cn_alteration_df, mutation_df], axis=1)\n",
    "\n",
    "# Example usage\n",
    "print(\"Concatenated DataFrame:\")\n",
    "print(concatenated_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36e4f1",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Function build_denoising_autoencoder:\n",
    "\n",
    "Constructs a denoising autoencoder model using TensorFlow/Keras.\n",
    "Adds Gaussian noise to the input data, defines encoder and decoder layers, and compiles the model.\n",
    "Trains the autoencoder on X_resampled data.\n",
    "Returns the trained autoencoder model, encoder model (up to the bottleneck layer), and DataFrame encoded_df containing encoded data.\n",
    "Concatenation of Multiomics Datasets:\n",
    "\n",
    "After dimensionality reduction using the encoder, concatenate the encoded data (encoded_df) with other omics datasets (transcriptomics_df, proteomics_df, etc.).\n",
    "Adjust axis=1 in pd.concat according to how you want to merge the datasets (by columns in this case).\n",
    "Example Usage:\n",
    "\n",
    "Replace X_resampled with your actual input data.\n",
    "Use the returned models (autoencoder, encoder) and encoded data (encoded_df) for further analysis or modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a892a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# Assuming you have X_train, Y_train, X_test, Y_test defined\n",
    "\n",
    "# Combine X_train and Y_train into a single DataFrame\n",
    "train_data = pd.concat([pd.DataFrame(X_train), pd.DataFrame(Y_train)], axis=1)\n",
    "\n",
    "# Define the neural network model\n",
    "def create_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(params['units1']), input_shape=(X_train.shape[1],), activation=params['activation']))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "    model.add(Dense(int(params['units2']), activation=params['activation']))\n",
    "    model.add(Dropout(params['dropout2']))\n",
    "    model.add(Dense(int(params['units3']), activation=params['activation']))\n",
    "    model.add(Dropout(params['dropout3']))\n",
    "    model.add(Dense(int(params['units4']), activation=params['activation']))\n",
    "    model.add(Dropout(params['dropout4']))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    elif params['optimizer'] == 'sgd':\n",
    "        optimizer = SGD(learning_rate=params['learning_rate'])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define the objective function for hyperopt\n",
    "def objective(params):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    validation_losses = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(train_data):\n",
    "        X_train_fold, X_val_fold = train_data.iloc[train_index, :-1], train_data.iloc[val_index, :-1]\n",
    "        Y_train_fold, Y_val_fold = train_data.iloc[train_index, -1], train_data.iloc[val_index, -1]\n",
    "\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold = scaler.transform(X_val_fold)\n",
    "\n",
    "        # Create and compile the model\n",
    "        model = create_model(params)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(\n",
    "            X_train_fold, Y_train_fold,\n",
    "            epochs=50,\n",
    "            batch_size=int(params['batch_size']),\n",
    "            validation_data=(X_val_fold, Y_val_fold),\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=int(params['early_stopping_patience']), restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = model.evaluate(X_val_fold, Y_val_fold, verbose=0)\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "    return {'loss': np.mean(validation_losses), 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space = {\n",
    "    'units1': hp.quniform('units1', 32, 512, 32),\n",
    "    'dropout1': hp.uniform('dropout1', 0.0, 0.5),\n",
    "    'units2': hp.quniform('units2', 32, 512, 32),\n",
    "    'dropout2': hp.uniform('dropout2', 0.0, 0.5),\n",
    "    'units3': hp.quniform('units3', 32, 512, 32),\n",
    "    'dropout3': hp.uniform('dropout3', 0.0, 0.5),\n",
    "    'units4': hp.quniform('units4', 32, 512, 32),\n",
    "    'dropout4': hp.uniform('dropout4', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-2)),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 128, 16),\n",
    "    'early_stopping_patience': hp.quniform('early_stopping_patience', 5, 20, 1),\n",
    "    'activation': hp.choice('activation', ['relu', 'tanh', 'sigmoid']),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'sgd'])\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print(\"Best hyperparameters found: \", best)\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "best_params = {\n",
    "    'units1': int(best['units1']),\n",
    "    'dropout1': best['dropout1'],\n",
    "    'units2': int(best['units2']),\n",
    "    'dropout2': best['dropout2'],\n",
    "    'units3': int(best['units3']),\n",
    "    'dropout3': best['dropout3'],\n",
    "    'units4': int(best['units4']),\n",
    "    'dropout4': best['dropout4'],\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'batch_size': int(best['batch_size']),\n",
    "    'early_stopping_patience': int(best['early_stopping_patience']),\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'][best['activation']],\n",
    "    'optimizer': ['adam', 'sgd'][best['optimizer']]\n",
    "}\n",
    "\n",
    "# Training the final model with the best hyperparameters\n",
    "X_train_final, X_val_final, Y_train_final, Y_val_final = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_final = scaler.fit_transform(X_train_final)\n",
    "X_val_final = scaler.transform(X_val_final)\n",
    "\n",
    "# Create and compile the model\n",
    "final_model = create_model(best_params)\n",
    "\n",
    "# Train the model\n",
    "final_model.fit(\n",
    "    X_train_final, Y_train_final,\n",
    "    epochs=50,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    validation_data=(X_val_final, Y_val_final),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=best_params['early_stopping_patience'], restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "Y_test_pred = final_model.predict(X_test_scaled).ravel()  # Reshape to 1D array\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n",
    "mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "mae = mean_absolute_error(Y_test, Y_test_pred)\n",
    "evs = explained_variance_score(Y_test, Y_test_pred)\n",
    "r_squared = r2_score(Y_test, Y_test_pred)\n",
    "q_squared = calculate_q_squared(Y_test, Y_test_pred)\n",
    "\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test MAE: {mae}\")\n",
    "print(f\"Test Explained Variance: {evs}\")\n",
    "print(f\"Test R-squared: {r_squared}\")\n",
    "print(f\"Test Q squared: {q_squared}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b06596",
   "metadata": {},
   "source": [
    "Explanation of the Code:\n",
    "Hyperopt Integration: The objective function now evaluates the model using 10-fold cross-validation, calculates the mean validation loss, and returns it as the objective value.\n",
    "Parameter Space: Defined the hyperparameter space using Hyperopt's hp module.\n",
    "Optimization: Used fmin to run the optimization process with 50 evaluations.\n",
    "Best Parameters: After finding the best hyperparameters, the model is retrained using the entire training set and the best hyperparameters.\n",
    "Evaluation: The final model is evaluated on the test set using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_smiles_excel(excel_file, output_folder, num_conformers=10):\n",
    "    def generate_conformers(smiles, num_conformers=10):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            AllChem.EmbedMultipleConfs(mol, numConfs=num_conformers)\n",
    "        return mol\n",
    "\n",
    "    def clean_filename(filename):\n",
    "        # Remove problematic characters from the filename\n",
    "        return \"\".join(c for c in filename if c.isalnum() or c in ['_', '-'])\n",
    "\n",
    "    def save_sdf(mol, folder, filename_prefix):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        drug_name = mol.GetProp('Drug Name')\n",
    "        clean_drug_name = clean_filename(drug_name)  # Clean the drug name\n",
    "        for i, conf in enumerate(mol.GetConformers()):\n",
    "            filename = os.path.join(folder, f\"{clean_drug_name}_{filename_prefix}_{i+1}.sdf\")\n",
    "            writer = Chem.SDWriter(filename)\n",
    "            writer.write(mol, confId=conf.GetId())\n",
    "            writer.close()\n",
    "\n",
    "    # Read SMILES IDs and drug names from an Excel sheet\n",
    "    df = pd.read_excel(excel_file)\n",
    "\n",
    "    # Iterate over SMILES IDs and drug names in the Excel sheet\n",
    "    for index, row in df.iterrows():\n",
    "        smiles_id = row['SMILES ID']\n",
    "\n",
    "        mol = generate_conformers(smiles_id, num_conformers)\n",
    "\n",
    "        if mol is not None:\n",
    "            mol.SetProp('Drug Name', str(row['Drug Name']))\n",
    "            save_sdf(mol, output_folder, f\"conformers_{index+1}_2d\")\n",
    "\n",
    "            img = Draw.MolsToGridImage([mol], molsPerRow=5, subImgSize=(300, 300))\n",
    "            display(img)\n",
    "        else:\n",
    "            drug_name = row['Drug Name']\n",
    "            clean_drug_name = clean_filename(drug_name)\n",
    "            print(f\"Error: Unable to create molecule for drug '{clean_drug_name}'\")\n",
    "\n",
    "# Usage\n",
    "excel_file = \"SMILES.xlsx\"  # Replace with your Excel file path\n",
    "output_folder = \"conformers_output\"\n",
    "process_smiles_excel(excel_file, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816cf10",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Function Definition: The entire script is wrapped into a single function called process_smiles_excel.\n",
    "Inner Functions: Helper functions generate_conformers, clean_filename, and save_sdf are defined inside the main function to keep the scope local.\n",
    "Parameters: The main function process_smiles_excel takes three parameters: excel_file, output_folder, and num_conformers with a default value of 10.\n",
    "Usage: At the end, the function is called with the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb141bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cfcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487f187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7fd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c025cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
